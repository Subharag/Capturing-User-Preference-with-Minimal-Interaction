{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation,Dropout,Dense, Flatten,BatchNormalization,Conv2D,MaxPool2D,Input,Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices=tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs available: \", len(physical_devices))\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=250\n",
    "batch_size=256\n",
    "margin=1 #Margin for constrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainx1.npy', 'rb') as f:\n",
    "    a = np.load(f)\n",
    "with open('trainx2.npy', 'rb') as f:\n",
    "    b = np.load(f)\n",
    "with open('labels.npy', 'rb') as f:\n",
    "    c = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1,x_val1,x_test1=a[:980000],a[980000:1190000],a[1190000:]\n",
    "x_train2,x_val2,x_test2=b[:980000],b[980000:1190000],b[1190000:]\n",
    "labels_train,labels_val,labels_test=c[:980000],c[980000:1190000],c[1190000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Similarity Measure Code using Euclidean Distance\n",
    "#Euclidean distance = sqrt(sum(square(t1-t2))) where t1 and t2 are tensors\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x,y=vects\n",
    "    sum_square=tf.math.reduce_sum(tf.math.square(x-y),axis=1,keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square,tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Network\n",
    "embedding_network=Sequential([\n",
    "    Conv2D(filters=64,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding=\"same\", input_shape=(28,28,1)), #32x32x32\n",
    "    MaxPool2D(pool_size=(3,3),strides=1), #16x16x32\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\",kernel_initializer='he_uniform',padding=\"same\"), #16x16x64\n",
    "    MaxPool2D(pool_size=(3,3),strides=2), #8x8x64\n",
    "    Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\",kernel_initializer='he_uniform',padding=\"same\"),#8x8x64\n",
    "    MaxPool2D(pool_size=(2,2),strides=2), #4x4x128\n",
    "    Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",kernel_initializer='he_uniform',padding=\"same\"),#8x8x64\n",
    "    MaxPool2D(pool_size=(2,2),strides=2), #4x4x128\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dense(units=2,activation=\"tanh\"),\n",
    "])\n",
    "\n",
    "embedding_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=Input((28,28,1))\n",
    "input2=Input((28,28,1))\n",
    "\n",
    "tower_1=embedding_network(input1)\n",
    "tower_2=embedding_network(input2)\n",
    "\n",
    "\n",
    "merge_layer=Lambda(euclidean_distance)([tower_1,tower_2])\n",
    "normal_layer=BatchNormalization()(merge_layer)\n",
    "output_layer=Dense(1,activation='sigmoid')(normal_layer)\n",
    "siamese=keras.Model(inputs=[input1,input2],outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"Margin is an Integer which defines the baseline for distance for which pairs should be classified as dissimilar\n",
    "    \"\"\"\n",
    "    def contrastive_loss(y_true,y_pred):\n",
    "        \"\"\"Calc the loss as a floating point value\"\"\"\n",
    "        square_pred=tf.math.square(y_pred)\n",
    "        margin_square=tf.math.square(tf.math.maximum(margin-(y_pred),0))\n",
    "        return tf.math.reduce_mean((1-y_true)*square_pred+(y_true)*margin_square)\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "#siamese.compile(loss=loss(margin=margin),optimizer=opt,metrics=[\"accuracy\"])\n",
    "siamese.compile(loss=\"binary_crossentropy\",optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= siamese.fit(\n",
    "    [x_train1,x_train2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val1,x_val2],labels_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=siamese.evaluate([x_test1,x_test2],labels_test)\n",
    "print(\"test loss,test accuracy\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese.save('siamese.h5')\n",
    "embedding_network.save('embed.h5')\n",
    "path_siamese='Weights_folder/Weights_siamese'\n",
    "path_model='Weights_folder/Weights_model'\n",
    " \n",
    "# save\n",
    "embedding_network.save_weights(path_model)\n",
    "siamese.save_weights(path_siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Accuracy and Loss data\n",
    "with open('train_acc.npy', 'wb') as f:\n",
    "    np.save(f, history.history[\"accuracy\"])\n",
    "    print(\"Train accuracy\")\n",
    "\n",
    "with open('val_acc.npy', 'wb') as f:\n",
    "    np.save(f, history.history[\"val_accuracy\"])\n",
    "    print(\"Validation accuracy\")\n",
    "    \n",
    "with open('train_loss.npy', 'wb') as f:\n",
    "    np.save(f, history.history[\"loss\"])\n",
    "    print(\"Train Loss\")\n",
    "\n",
    "with open('val_loss.npy', 'wb') as f:\n",
    "    np.save(f, history.history[\"val_loss\"])\n",
    "    print(\"Validation loss\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
